<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>The Ferret Genetic Algorithm</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html,frames,3 --> 
<meta name="src" content="QubistHTML.tex"> 
<meta name="date" content="2012-02-01 01:22:00"> 
<link rel="stylesheet" type="text/css" href="QubistHTML.css"> 
</head><body 
>
<!--l. 380--><div class="crosslinks"><p class="noindent">[<a 
href="QubistHTMLse3.html" >next</a>] [<a 
href="QubistHTMLse1.html" >prev</a>] [<a 
href="QubistHTMLse1.html#tailQubistHTMLse1.html" >prev-tail</a>] [<a 
href="#tailQubistHTMLse2.html">tail</a>] [<a 
href="QubistHTMLch2.html#QubistHTMLse2.html" >up</a>] </p></div>
<h3 class="sectionHead"><span class="titlemark">2.2   </span> <a 
 id="x8-80002.2"></a>The Ferret Genetic Algorithm</h3>
<a 
 id="dx8-8001"></a>
<!--l. 384--><p class="noindent" >
     <div class="quote">
     <!--l. 385--><p class="noindent" ><span 
class="cmti-10">&#8216;Owing</span>
     <span 
class="cmti-10">to</span>
     <span 
class="cmti-10">this</span>
     <span 
class="cmti-10">struggle</span>
     <span 
class="cmti-10">for</span>
     <span 
class="cmti-10">life,</span>
     <span 
class="cmti-10">any</span>
     <span 
class="cmti-10">variation,</span>
     <span 
class="cmti-10">however</span>
     <span 
class="cmti-10">slight</span>
     <span 
class="cmti-10">and</span>
     <span 
class="cmti-10">from</span>
     <span 
class="cmti-10">whatever</span>
     <span 
class="cmti-10">cause</span>
     <span 
class="cmti-10">proceeding,</span>
     <span 
class="cmti-10">if</span>
     <span 
class="cmti-10">it</span>
     <span 
class="cmti-10">be</span>
     <span 
class="cmti-10">in</span>
     <span 
class="cmti-10">any</span>
     <span 
class="cmti-10">degree</span>
     <span 
class="cmti-10">profitable</span>
     <span 
class="cmti-10">to</span>
     <span 
class="cmti-10">an</span>
     <span 
class="cmti-10">individual</span>
     <span 
class="cmti-10">of</span>
     <span 
class="cmti-10">any</span>
     <span 
class="cmti-10">species,</span>
     <span 
class="cmti-10">in</span>
     <span 
class="cmti-10">its</span>
                                                                                         

                                                                                         
     <span 
class="cmti-10">infinitely</span>
     <span 
class="cmti-10">complex</span>
     <span 
class="cmti-10">relationship</span>
     <span 
class="cmti-10">to</span>
     <span 
class="cmti-10">other</span>
     <span 
class="cmti-10">organic</span>
     <span 
class="cmti-10">beings</span>
     <span 
class="cmti-10">and</span>
     <span 
class="cmti-10">to</span>
     <span 
class="cmti-10">external</span>
     <span 
class="cmti-10">nature,</span>
     <span 
class="cmti-10">will</span>
     <span 
class="cmti-10">tend</span>
     <span 
class="cmti-10">to</span>
     <span 
class="cmti-10">the</span>
     <span 
class="cmti-10">preservation</span>
     <span 
class="cmti-10">of</span>
     <span 
class="cmti-10">that</span>
     <span 
class="cmti-10">individual,</span>
     <span 
class="cmti-10">and</span>
     <span 
class="cmti-10">will</span>
     <span 
class="cmti-10">generally</span>
     <span 
class="cmti-10">be</span>
     <span 
class="cmti-10">inherited</span>
     <span 
class="cmti-10">by</span>
     <span 
class="cmti-10">its</span>
     <span 
class="cmti-10">offspring.&#8217;</span><br />
     -Charles
     Darwin,
     <span 
class="cmti-10">The</span>
     <span 
class="cmti-10">Origin</span>
     <span 
class="cmti-10">of</span>
     <span 
class="cmti-10">Species</span></div>
<a 
 id="dx8-8002"></a>
<a 
 id="dx8-8003"></a>
<a 
 id="dx8-8004"></a>
<!--l. 392--><p class="noindent" >Genetic algorithms (GAs), like Ferret, are an important class of algorithms for global optimization that work
in analogy to biological evolution. Evolution is biology&#8217;s optimization strategy of choice, in which
organisms evolve and continually improve their own designs as they struggle to survive. It is a common
misconception that evolution is a random process that is dominated by mutation. It is more correct
to regard evolution as a directed stochastic search, where random processes like mutation work in
concert with non-random processes like genetic crossover (mating) and natural selection to optimize the
genome of a species. GAs use the principles of biological evolution as the basis for a global optimization
scheme.
                                                                                         

                                                                                         
<!--l. 394--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.1   </span> <a 
 id="x8-90002.2.1"></a>History of the Ferret Project</h4>
<a 
 id="dx8-9001"></a>
<a 
 id="dx8-9002"></a>
<!--l. 399--><p class="noindent" >Sometimes I find that a historical perspective is useful to put a project in context, and I will briefly recollect Ferret&#8217;s
seven-year history in this short section. Readers who prefer to skip the history and move on to the technical details
should skip to Section <a 
href="#x8-100002.2.2">2.2.2<!--tex4ht:ref: sec:FerretFeatures --></a>.
<a 
 id="dx8-9003"></a>
<a 
 id="dx8-9004"></a>
<a 
 id="dx8-9005"></a>
<a 
 id="dx8-9006"></a>
<a 
 id="dx8-9007"></a>
<a 
 id="dx8-9008"></a>
<!--l. 407--><p class="noindent" >The Ferret project began in 2002 while I was a Plaskett Research Fellow with the Canadian National Research
Council in Victoria, British Columbia. At this time, I was grappling with a particularly difficult data-modeling
problem, and I was getting nowhere using local optimizers, grid searches and random search techniques. My
frustration was exacerbated by my attempts to visualize the results from my grid searches, and I recall producing
some of the most confusing and least useful plots that I have ever generated. I distinctly remember an evening when
I realized - in all seriousness - that every major problem that I had worked on since I was a Master&#8217;s student had
amounted to essentially the same problem: that of testing complicated theoretical models in astrophysics by
applying them to even more complicated and noisy astronomical data sets. Furthermore, I realized
that all of these problems were just difficult parameter search and optimization problems, and that a
sufficiently powerful code for performing these tasks would go a long way in terms of solving all of my
data-modeling problems at once. I turned to GAs because more standard techniques had failed miserably on
my problem, and I built my first rather naive GA in about an hour. Nevertheless, I was delighted to
discover that this rather primitive code was outperforming everything that I had tried up to that
point. I was hooked, and I have spent much of my free time building evermore powerful GAs since that
day.
<a 
 id="dx8-9009"></a>
<a 
 id="dx8-9010"></a>
<a 
 id="dx8-9011"></a>
<a 
 id="dx8-9012"></a>
<a 
 id="dx8-9013"></a>
<a 
 id="dx8-9014"></a>
<!--l. 415--><p class="noindent" >I have never really considered Ferret (or later Qubist) to be part of my work as an astrophysicist, but rather a
separate exploration to satisfy my curiosity, which was fortuitously well-aligned with my scientific goals. So
I had an interesting new hobby, and Ferret really took on a life of its own about six months later.
Ferret-1 was working quite nicely as a multi-objective code towards the end of 2003, and I had also
built versions for Scilab (http://www.scilab.org) and Octave (http://www.gnu.org/software/octave),
which are essentially free ports of the MATLAB language, with minor differences. I also built a parallel
Java-based GA called &#8216;Finch&#8217; (after Darwin&#8217;s finches), which was somewhat different from Ferret, but
about as powerful as Ferret was at the time. Eventually, this period of exploration ended and I decided
that it was better to focus on a single software package rather than two. I chose Ferret over Finch
                                                                                         

                                                                                         
because I realized that I could develop Ferret more rapidly than Finch due to my experience with
MATLAB, and because I felt that Ferret had the greater potential to find users in the scientific computing
community, where MATLAB and MATLAB-like languages are used extensively. I abandoned Finch, although
some of the ideas and algorithms from this code influenced Ferret significantly due to their concurrent
development.
<a 
 id="dx8-9015"></a>
<a 
 id="dx8-9016"></a>
<a 
 id="dx8-9017"></a>
<a 
 id="dx8-9018"></a>
<!--l. 421--><p class="noindent" >I almost completely re-built Ferret in 2004-2005, after moving to Winnipeg, Manitoba to start my current position
at the University of Manitoba. I had a lot of new responsibilities - most notably teaching - and I found it necessary
to abandon the Scilab port of Ferret, as well as the Octave version, which was already lagging behind by
the time I moved. I chose the MATLAB version because I had my sights set on applications in both
academia <span 
class="cmti-10">and </span>industry, where MATLAB is the gold standard. This was an innovative time in Ferret&#8217;s
development, and the time period when I developed Ferret&#8217;s linkage-learning features. I actually developed two
independent and successively more powerful linkage-learning systems during this period, first for Ferret-2
and then for Ferret-3, which each required quite a dramatic re-structuring of the code. By the end of
2005, Ferret had grown to several times its previous size, and there was very little left of the original
code.
<a 
 id="dx8-9019"></a>
<a 
 id="dx8-9020"></a>
<a 
 id="dx8-9021"></a>
<a 
 id="dx8-9022"></a>
<!--l. 427--><p class="noindent" >Around 2006, I began to put greater emphasis on developing an intuitive user interface and visualization
system. This was motivated by the fact that an increasing number of people were using the code for
problems of increasing complexity. Most of these early users were fellow physics and astronomy faculty
members, or students at the University of Manitoba. I quickly discovered that this group is extremely
good breaking any feature that is not absolutely robust, and Ferret went through an intense period of
enhancements, crash-proofing, and overall hardening. I owe this group of people a debt of gratitude
for their numerous bug reports and occasional feature requests. Moreover, the astronomy group was
extremely interested in graphics and visualization of Ferret&#8217;s parameter sets because of the very large
astrophysical data modeling projects that we were working on. As a result, my own interest in graphics and
visualization increased, and I developed the visualization system that is integrated with the present version of
the code. Finally, I built the current version of the code - Ferret-4 - in 2008. This new version was
highlighted by many improvements to the graphics and visualization system, the addition of a built-in
(and very easy to use) parallel computing system, the introduction of Ferret&#8217;s strategy parameter
auto-adaptation features, and user-modifiable &#8216;skins&#8217; for all of the major operating systems. The transition to
Ferret-4 was not as fundamental of a revision as Ferret-2 or Ferret-3 had been, but it included many
cumulative improvements that together were at least as important, and therefore an increment in the
version number was warranted. Incidentally, I began development of all of Qubist&#8217;s other optimizers and
supporting tools during the period from early 2007 - 2009, when Ferret-3 and 4 were under active
development.
<a 
 id="dx8-9023"></a>
<a 
 id="dx8-9024"></a>
<a 
 id="dx8-9025"></a>
                                                                                         

                                                                                         
<!--l. 433--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.2   </span> <a 
 id="x8-100002.2.2"></a>Ferret&#8217;s Features</h4>
<a 
 id="dx8-10001"></a>
<!--l. 437--><p class="noindent" >Ferret contains many enhancements that go far beyond a typical genetic algorithm and existing optimization
packages. These enhancements include the following:
     <ul class="itemize1">
     <li class="itemize">
     <a 
 id="dx8-10002"></a>
     <a 
 id="dx8-10003"></a>Multi-objective
     search
     and
     parameter
     space
     mapping:
     Ferret
     is
     designed
     as
     a
     powerful
     multi-objective
     optimizer,
     which
     very
     effectively
     maps
     out
     and
     displays
     trade-off
     surfaces
     between
     multiple
     objective
     functions.
     This
     ability
     allows
     you
     to
     understand
     the
     compromises
     that
     must
                                                                                         

                                                                                         
     be
     made
     between
     several
     conflicting
     objectives,
     and
     can
     sometimes
     help
     to
     refine
     the
     true
     objectives
     of
     your
     problem
     if
     you
     discover
     that
     you
     prefer
     one
     part
     of
     the
     trade-off
     surface
     over
     another.
     The
     same
     powerful
     parameter
     space
     mapping
     machinery
     used
     for
     multi-objective
     problems
     can
     also
     be
     unleashed
     on
                                                                                         

                                                                                         
     problems
     with
     only
     a
     single
     objective.
     It
     is
     often
     useful
     to
     allow
     some
     tolerance
     on
     the
     objective
     function
     (or
     fitness
     function),
     so
     that
     Ferret
     returns
     the
     true
     optimum,
     plus
     a
     scattering
     of
     points
     within
     the
     tolerance.
     Why
     is
     this
     useful?
     In
     any
     data-modeling
     problem,
     one
     is
     always
     concerned
                                                                                         

                                                                                         
     with
     both
     the
     best-fit
     and
     the
     errors
     on
     the
     fit.
     If
     one
     is
     minimizing
     a
     reduced
     <span 
class="cmmi-10">&#x03C7;</span><sup><span 
class="cmr-7">2</span></sup>
     function,
     for
     example,
     it
     is
     easy
     to
     tell
     Ferret
     to
     minimize
     the
     function,
     but
     also
     map
     out
     the
     entire
     region
     within
     &#x0394;<span 
class="cmmi-10">&#x03C7;</span><sub><span 
class="cmmi-7">reduced</span></sub><sup><span 
class="cmr-7">2</span></sup> = 1
     (say)
     of
     the
     minimum
     to
     map
     the
     1-sigma
     confidence
                                                                                         

                                                                                         
     region.
     In
     effect,
     this
     provides
     built-in
     error
     analysis.
     </li>
     <li class="itemize">
     <a 
 id="dx8-10004"></a>
     <a 
 id="dx8-10005"></a>Built-in
     parallelization,
     which
     does
     <span 
class="cmti-10">not</span>
     require
     the
     user
     to
     purchase
     MATLAB&#8217;s
     parallel
     computing
     toolbox
     or
     use
     any
     other
     software
     not
     included
     with
     Qubist.
     </li>
     <li class="itemize">
     <a 
 id="dx8-10006"></a>
     <a 
 id="dx8-10007"></a>Simple
     handling
     of
     discrete
     and
     cyclic
     parameters.
                                                                                         

                                                                                         
     </li>
     <li class="itemize">
     <a 
 id="dx8-10008"></a>
     Automatic
     zooming:
     Ferret
     can
     be
     configured
     to
     automatically
     zoom
     in
     on
     the
     global
     solution
     as
     the
     population
     converges
     toward
     the
     optimal
     set.
     This
     allows
     the
     code
     to
     find
     the
     optimal
     set
     efficiently
     and
     to
     high
     accuracy.
     </li>
     <li class="itemize">
     <a 
 id="dx8-10009"></a>
     Critical
     Parameter
     Detection:
                                                                                         

                                                                                         
     Ferret
     contains
     a
     unique
     system
     for
     <span 
class="cmti-10">Critical</span>
     <span 
class="cmti-10">Parameter</span>
     <span 
class="cmti-10">Detection</span>
     (CPD),
     which
     allows
     the
     code
     to
     dynamically
     monitor
     the
     set
     of
     parameters
     during
     a
     run
     to
     determine
     which
     ones
     are
     actually
     important
     to
     the
     optimization.
     If
     a
     parameter
     is
     determined
     to
     be
     unimportant,
     it
     is
     explicitly
     ignored,
     thus
     reducing
                                                                                         

                                                                                         
     the
     size
     of
     the
     parameter
     space.
     </li>
     <li class="itemize">
     <a 
 id="dx8-10010"></a>
     Linkage-Learning:
     A
     novel
     <span 
class="cmti-10">linkage-learning</span>
     algorithm
     that
     is
     designed
     to
     reduce
     a
     complex,
     multi-parameter
     problem
     to
     a
     natural
     set
     of
     smaller
     sub-problems
     whenever
     such
     a
     reduction
     is
     possible.
     These
     simpler
     sub-problems
     are
     discovered
     experimentally
     by
     Ferret
     during
     the
                                                                                         

                                                                                         
     process
     of
     optimization,
     and
     strongly
     partitioned
     sub-problems
     evolve
     almost
     independently
     during
     a
     run.
     This
     process
     is
     dynamic.
     Parameters
     that
     appear
     linked
     at
     the
     start
     of
     a
     run
     may
     not
     appear
     linked
     at
     the
     end,
     when
     most
     trial
     solutions
     may
     be
     nearly
     optimal.
     Conversely,
     new
     links
     can
     also
     arise
                                                                                         

                                                                                         
     as
     the
     code
     explores
     previously
     uncharted
     regions
     of
     parameter
     space.
     The
     ability
     to
     partition
     a
     complicated
     problem
     into
     <span 
class="cmti-10">natural</span>
     sub-problems
     is
     crucial
     to
     the
     successful
     optimization
     of
     large
     problems.
     A
     difficult
     100-parameter
     problem
     with
     many
     local
     minima
     is
     probably
     unsolvable
     on
     its
     own,
     but
     it
     becomes
     quite
     tractable
                                                                                         

                                                                                         
     <span 
class="cmti-10">if</span>
     it
     can
     be
     partitioned
     into
     (say)
     ten
     sub-problems
     (or
     building
     blocks)
     with
     ten
     parameters
     each.
     An
     interesting
     feature
     of
     Ferret&#8217;s
     linkage-learning
     system
     is
     that
     the
     linkages
     discovered
     are
     entirely
     insensitive
     to
     scale.
     Two
     sub-problems
     that
     are
     orders
     of
     magnitude
     different
     in
     importance
     are
     discovered
     at
     the
     same
                                                                                         

                                                                                         
     rate,
     so
     that
     Ferret
     can
     solve
     all
     of
     the
     sub-problems
     correctly
     and
     simultaneously,
     rather
     than
     one
     at
     a
     time
     in
     order
     of
     significance.
     This
     ability
     allows
     Ferret
     to
     discover
     the
     true,
     globally
     optimal
     solution
     or
     solution
     set,
     even
     when
     applied
     to
     problems
     with
     very
     poorly
     scaled
     building
     blocks.
                                                                                         

                                                                                         
     </li>
     <li class="itemize">
     <a 
 id="dx8-10011"></a>
     Advanced
     Lethal
     Suppression
     (ALS):
     &#8216;Lethals&#8217;
     are
     poor
     quality
     solutions
     that
     are
     sometimes
     obtained
     from
     the
     crossover
     of
     two
     good
     parent
     solutions.
     Lethals
     degrade
     the
     performance
     of
     a
     GA,
     since
     they
     represent
     wasted
     solutions
     that
     are
     likely
     to
     be
     be
     discarded
     during
     the
     next
     round
                                                                                         

                                                                                         
     of
     tournament
     selection.
     When
     enabled,
     Ferret&#8217;s
     ALS
     algorithm
     allows
     the
     code
     to
     learn
     what
     parts
     of
     the
     parameter
     space
     are
     likely
     to
     contain
     lethals,
     and
     avoid
     these
     regions
     during
     crossovers.
     </li>
     <li class="itemize">
     <a 
 id="dx8-10012"></a>
     Strategy
     Auto-Adaptation:
     Ferret
     contains
     a
     powerful
     algorithm
     that
     monitors
     its
     progress
     and
     uses
                                                                                         

                                                                                         
     this
     information
     to
     automatically
     adapt
     several
     of
     its
     most
     important
     control
     parameters,
     including
     the
     mutation
     scale,
     size
     scale
     of
     crossover
     events,
     and
     several
     others.
     If
     these
     parameters
     are
     set
     poorly
     by
     the
     user,
     Ferret
     will
     quickly
     and
     dynamically
     adapt
     them
     to
     improve
     the
     search.
     </li>
     <li class="itemize">
                                                                                         

                                                                                         
     <a 
 id="dx8-10013"></a>
     Pausing,
     stopping,
     and
     resuming
     runs:
     Ferret
     produces
     &#8216;History&#8217;
     files
     every
     few
     generations,
     so
     that
     no
     information
     is
     lost
     if
     the
     user
     chooses
     to
     suspend
     a
     run,
     and
     only
     minimal
     information
     is
     lost
     if
     MATLAB
     or
     the
     computer
     system
     happens
     to
     crash.
     </li>
     <li class="itemize">
     <a 
 id="dx8-10014"></a>
     <a 
 id="dx8-10015"></a><a 
 id="dx8-10016"></a>
                                                                                         

                                                                                         
     Analysis
     and
     integration
     of
     other
     Qubist
     optimizers:
     You
     can
     analyze
     a
     Ferret
     run
     while
     it
     is
     in
     progress
     or
     after
     it
     is
     complete
     to
     compute
     the
     optimal
     set
     of
     solutions.
     These
     solutions
     can
     be
     fine-tuned
     by
     your
     choice
     of
     three
     different
     polishers
     with
     a
     single
     button
     click.
                                                                                         

                                                                                         
     </li>
     <li class="itemize">
     <a 
 id="dx8-10017"></a>
     Integrated
     visualization:
     You
     can
     visualize
     the
     progress
     of
     a
     Ferret
     run
     while
     it
     is
     in
     progress
     through
     an
     intuitive
     and
     informative
     interface.
     A
     more
     sophisticated
     interface
     is
     available
     after
     the
     run
     is
     analyzed
     to
     find
     the
     optimal
     set.
     The
     analysis
     window
     contains
     various
     graphics
                                                                                         

                                                                                         
     options
     to
     tease
     out
     interesting
     features
     from
     the
     optimal
     set.
     These
     features
     include
     two
     and
     three-dimensional
     scatter
     plots,
     image
     plots,
     contour
     plots,
     and
     user-defined
     graphics.
     <a 
 id="dx8-10018"></a>You
     can
     also
     &#8216;paint&#8217;
     interesting
     regions
     of
     the
     parameter
     space
     and
     easily
     select
     different
     two
     and
     three-dimensional
     projections
     to
     explore
     and
     visualize
     where
                                                                                         

                                                                                         
     the
     painted
     solutions
     reside
     in
     a
     high-dimensional
     space.
     Of
     course,
     you
     can
     turn
     off
     Ferret&#8217;s
     graphical
     user
     interface
     if
     desired,
     and
     Qubist&#8217;s
     visualization
     tools
     can
     still
     be
     applied
     to
     the
     final
     solution
     set
     at
     the
     end
     of
     the
     run.</li></ul>
<a 
 id="dx8-10019"></a>
<a 
 id="dx8-10020"></a>
<a 
 id="dx8-10021"></a>
<a 
 id="dx8-10022"></a>
<a 
 id="dx8-10023"></a>
<!--l. 458--><p class="noindent" >In conclusion, it should be clear that Ferret bears little resemblance to traditional GAs like the original version
presented by John Holland in 1975 [<a 
href="QubistHTMLli1.html#Xholland75">Holland</a>,&#x00A0;<a 
href="QubistHTMLli1.html#Xholland75">1975</a>]. Ferret is an integrated package for parameter search, global
optimization, data-modeling, and visualization, based on an evolutionary algorithm that is much more sophisticated
                                                                                         

                                                                                         
than Holland&#8217;s original GA or subsequent GAs like those discussed by David Goldberg is his authoritative 1989
monograph on the subject [<a 
href="QubistHTMLli1.html#Xgoldberg89">Goldberg</a>,&#x00A0;<a 
href="QubistHTMLli1.html#Xgoldberg89">1989</a>]. Ferret works with real-valued parameters and self-adaptive mutation
scales, much like an Evolution Strategies (ES) code, but Ferret remains closer to a GA than an ES code due to its
emphasis on crossovers. As a multi-objective GA, Ferret has some features in common with other
multi-objective GAs that have appeared in the literature (e.g. <a 
href="QubistHTMLli1.html#Xfonseca93">Fonseca &amp; Fleming</a>&#x00A0;[<a 
href="QubistHTMLli1.html#Xfonseca93">1993</a>],&#x00A0;<a 
href="QubistHTMLli1.html#Xhorn94">Horn et
al.</a>&#x00A0;[<a 
href="QubistHTMLli1.html#Xhorn94">1994</a>]). However, Ferret blends these multi-objective capabilities with powerful linkage learning
techniques that are in the same spirit (but quite different in their internal workings) as those discussed by
<a 
href="QubistHTMLli1.html#Xgoldberg02">Goldberg</a>&#x00A0;[<a 
href="QubistHTMLli1.html#Xgoldberg02">2002</a>]. Visualization is central to the package because Ferret is designed to help you to discover the
meaning of your results, and seeing the structure of the multi-dimensional optimal set is the first step in
this process. Ferret&#8217;s built-in parallelization system takes advantage of modern multi-CPU machines
and clusters. It does not require any knowledge of parallel computing - literally, you just select the
number of &#8216;worker nodes&#8217; that you want from a drop-down menu, click a start button, and the rest is
automatic.
<a 
 id="dx8-10024"></a>
<!--l. 461--><p class="noindent" >I believe that Ferret is the only GA in existence (at the time of this writing) that contains the powerful combination
of features discussed above. Ferret is designed for real-world research problems, and as an active scientific
researcher, I have been the code&#8217;s harshest critic and most demanding user. I have spent much of
the past seven years finding problems, fixing them, developing new techniques, and pushing Ferret&#8217;s
limits to handle massive-scale optimization and data-modeling problems on fairly modest computing
hardware.
<!--l. 463--><p class="noindent" >Most of this manual will be devoted to discussing Ferret and how to configure its advanced features to suit specific
types of problems. The chapters on SAMOSA, Anvil, and Locust are much shorter because these optimizers are
much simpler than Ferret, but they share a great deal with Ferret in terms of their configuration options and usage.
All of the other optimizers understand fitness functions and setup files made for Ferret. Therefore, it is a fair
statement that once you learn to use Ferret, you will have an excellent working knowledge of the entire Qubist
package.
<!--l. 466--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.3   </span> <a 
 id="x8-110002.2.3"></a>Multi-Objective Optimization with Ferret</h4>
<a 
 id="dx8-11001"></a>
<a 
 id="dx8-11002"></a>
<!--l. 471--><p class="noindent" >Of all of the Qubist optimizers, Ferret has the most advanced and thoroughly tested features for multi-objective
optimization. Multi-objective genetic algorithms are wonderful tools for mapping out optimal sets and
multi-objective trade-off surfaces, but basic genetic algorithms are not well suited for this purpose.
<a 
 id="dx8-11003"></a>
<a 
 id="dx8-11004"></a>
<a 
 id="dx8-11005"></a>
<a 
 id="dx8-11006"></a>
<a 
 id="dx8-11007"></a>
<!--l. 478--><p class="noindent" >Consider so-called &#8216;scalarization&#8217; techniques, which attempt to solve a multi-objective problem with <span 
class="cmmi-10">N </span>objectives, by
converting it into an equivalent scalar-valued single objective problem:
<table 
class="equation"><tr><td><a 
 id="x8-11008r1"></a>
                                                                                         

                                                                                         
<center class="math-display" >
<img 
src="QubistHTML0x.png" alt="       N
f(x) = &#x2211;  w F = w &#x22C5;F,
       i=1  i i
" class="math-display" ></center></td><td class="equation-label">(2.1)</td></tr></table>
<!--l. 482--><p class="nopar" >
where <span 
class="cmmi-10">w </span>is a vector of weights such that <span 
class="cmsy-10">|</span><span 
class="cmmi-10">w</span><span 
class="cmsy-10">|</span>=1. The geometric interpretation is clear: optimization is performed in
the direction of <span 
class="cmmi-10">w</span>, and the Pareto front can be mapped by varying <span 
class="cmmi-10">w </span>over all possible values, <span 
class="cmti-10">provided that the</span>
<span 
class="cmti-10">trade-off surface is geometrically convex</span>. However, non-convex regions of the Pareto surface will be missed entirely
by this approach. Ferret does not use scalarization and performs very well on non-convex problems. This means
that Ferret can be used to explore more general fitness functions, without worrying about the issue of
convexivity.
<a 
 id="dx8-11009"></a>
<!--l. 486--><p class="noindent" >Let us examine the following problem as a trivial example of a non-convex multi-objective minimization problem on
the interval <span 
class="cmmi-10">&#x03B8; </span><span 
class="cmsy-10">&#x2208; </span>[0<span 
class="cmmi-10">,&#x03C0;&#x2215;</span>2]:
<table 
class="equation"><tr><td><a 
 id="x8-11010r2"></a>
<center class="math-display" >
<img 
src="QubistHTML1x.png" alt="       1+-k-cos2(2&#x03B8;)
R (&#x03B8;) =    1 + k    ; F1(&#x03B8;) = R cos&#x03B8;; F2 (&#x03B8;) = Rsin&#x03B8;,
" class="math-display" ></center></td><td class="equation-label">(2.2)</td></tr></table>
<!--l. 490--><p class="nopar" >
where <span 
class="cmmi-10">F</span><sub><span 
class="cmr-7">1</span></sub>(<span 
class="cmmi-10">&#x03B8;</span>) and <span 
class="cmmi-10">F</span><sub><span 
class="cmr-7">2</span></sub>(<span 
class="cmmi-10">&#x03B8;</span>) are functions to be minimized, and <span 
class="cmmi-10">k </span>is a constant. Figure <a 
href="#x8-110111">2.1<!--tex4ht:ref: fig:nonConvex --></a> shows the Pareto surface of
equation <a 
href="#x8-11010r2">2.2<!--tex4ht:ref: eq:nonConvex --></a> for constant <span 
class="cmmi-10">k </span>= 0 (left), <span 
class="cmmi-10">k </span>= 1 (middle), and <span 
class="cmmi-10">k </span>= 2 (right). Solutions from Ferret are
shown as blue dots that join together to form a continuous curve for <span 
class="cmmi-10">k </span>= 0 and <span 
class="cmmi-10">k </span>= 1, but the curve is
correctly broken into three isolated &#8216;islands&#8217; of solutions when <span 
class="cmmi-10">k </span>= 2. A scalarization method using
equation <a 
href="#x8-11008r1">2.1<!--tex4ht:ref: eq:scalarization --></a> only finds the convex portions of the curve populated with red dots or the endpoints
shown as red stars. We note that scalarization missed much of the Pareto surface for all values of <span 
class="cmmi-10">k</span>
shown.
<!--l. 493--><p class="noindent" ><hr class="figure"><div class="figure" 
>
                                                                                         

                                                                                         
<a 
 id="x8-110111"></a>
                                                                                         

                                                                                         
<div class="center" 
>
<!--l. 494--><p class="noindent" >

<!--l. 495--><p class="noindent" ><img 
src="figures/scalarizationExample/k0.png" alt="PIC"  
> <img 
src="figures/scalarizationExample/k1.png" alt="PIC"  
> <img 
src="figures/scalarizationExample/k2.png" alt="PIC"  
></div>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;2.1: </span><span  
class="content">Comparison of Ferret to a scalarization method for a simple non-convex problem with two
objectives (equation <a 
href="#x8-11010r2">2.2<!--tex4ht:ref: eq:nonConvex --></a>). Ferret maps a continuous trade-off surface shown in blue when <span 
class="cmmi-10">k </span>= 0 and <span 
class="cmmi-10">k </span>= 1,
but the Pareto surface is correctly broken into three isolated &#8216;islands&#8217; when <span 
class="cmmi-10">k </span>= 2. The red dots and red
stars are solutions found using a scalarization approach. Ferret maps the entire Pareto surface for all three
cases, but the scalarization method always misses non-convex parts of the Pareto surface.</span></div><!--tex4ht:label?: x8-110111 -->
                                                                                         

                                                                                         
<!--l. 501--><p class="noindent" ></div><hr class="endfigure">
<a 
 id="dx8-11012"></a>
<a 
 id="dx8-11013"></a>
<a 
 id="dx8-11014"></a>
<a 
 id="dx8-11015"></a>
<a 
 id="dx8-11016"></a>
<a 
 id="dx8-11017"></a>
<!--l. 509--><p class="noindent" >A single-objective GA (plus scalarization) is not ideal for mapping trade-off surfaces even if your problem is known
to be convex, and it is also not ideal for mapping multi-dimensional confidence intervals in data-modeling problems
(see Section <a 
href="#x8-100002.2.2">2.2.2<!--tex4ht:ref: sec:FerretFeatures --></a>) for the same underlying reason. Genetic algorithms naturally suffer from a well-known property
called genetic drift, which tends to cause a population to converge to some arbitrary point within the optimal set
rather than mapping it out. The basic problem is that genetic algorithms require a &#8216;crossover&#8217; step, in
which two solutions are combined to generate a new solution, and a selection step, which destroys the
weaker solution in favour of the stronger. Both of these operations tend to destroy the &#8216;diversity&#8217; of the
solution set - selection directly destroys a solution, but crossover is more subtle. It destroys diversity
because it is essentially an averaging operation that produces a &#8216;child&#8217; solution with properties that
are in some way intermediate between the parents. When this is done repeatedly, the solutions tend
toward uniformity - i.e. a single point in parameter space. In some ways, I find it strange that a class of
algorithms with this serious defect could become the basis for a powerful multi-objective code, but there
are well-known techniques that correct this problem. Multi-objective genetic algorithms like Ferret
require sophisticated schemes, usually based on a &#8216;niching&#8217; algorithm (see Section <a 
href="QubistHTMLse52.html#x78-1100008.9">8.9<!--tex4ht:ref: sec:par_niching --></a>), to &#8216;prop up&#8217;
the optimal set against over-convergence due to genetic drift. A good niching scheme can be quite
tricky to design, and this was one of the major successes of Ferret-1 - the first generation of the Ferret
code.
<a 
 id="dx8-11018"></a>
<h4 class="subsectionHead"><span class="titlemark">2.2.4   </span> <a 
 id="x8-120002.2.4"></a>Cyclic and Discrete Parameters:</h4>
<a 
 id="dx8-12001"></a>
<a 
 id="dx8-12002"></a>
<a 
 id="dx8-12003"></a>
<a 
 id="dx8-12004"></a>
<!--l. 520--><p class="noindent" >By default, Ferret is configured as a bounded optimizer, which searches a parameter space with well-defined minimum and maximum
boundaries in each parameter.<span class="footnote-mark"><a 
href="QubistHTML9.html#fn1x2"><sup class="textsuperscript">1</sup></a></span><a 
 id="x8-12005f1"></a> 
A bounded parameter space can be represented geometrically as an <span 
class="cmmi-10">n</span>-dimensional box, where <span 
class="cmmi-10">n </span>is the number
of parameters. However, it is possible to indicate that some parameters are cyclic, which tells the
code that the two opposite extremes of these parameter are really the same. Such parameters often
represent angles, where 0 and 2<span 
class="cmmi-10">&#x03C0; </span>radians normally represent the same angle. In such cases, Ferret joins
the corresponding opposite edges of the search box, so that it becomes geometrically equivalent to
an <span 
class="cmmi-10">n</span>-dimensional torus. While this may be difficult to visualize for more than two parameters, the
                                                                                         

                                                                                         
details are handled internally and transparently to the user once a parameter had been designated as
cyclic.
<a 
 id="dx8-12006"></a>
<a 
 id="dx8-12007"></a>
<!--l. 524--><p class="noindent" >Ferret can also handle parameters that are only allowed to take on only certain discrete values with
some uniform spacing. This is intended for real-valued parameters that represent the ordered state
of a system, but have underlying constraints that force them into discrete values. However, it is not
meant for combinatorial optimization problems, where a parameter represents an arbitrary state of the
system, without any natural ordering of the states. For example, an angle that is constrained to take
on discrete values in increments of ten degrees is fine, because the angles have a natural ordering.
On the other hand, a parameter that labels the cities on a map in a traveling salesman problem is
not an ordered discrete parameter and will not be handled as well by Ferret. This is not to say that
it is impossible to represent such a parameter in Ferret, or that the code won&#8217;t make any progress
on such a problem. Usually it will make reasonable progress anyway, but combinatorial optimization
techniques are better suited for these types of problems, and Ferret is not designed as a combinatorial
optimizer.
<h4 class="subsectionHead"><span class="titlemark">2.2.5   </span> <a 
 id="x8-130002.2.5"></a>Automatic Zooming:</h4>
<a 
 id="dx8-13001"></a>
<!--l. 530--><p class="noindent" >Ferret can be configured to dynamically zoom in on a problem&#8217;s high performance region - the region of parameter
space containing the currently best solution or solutions - as a run progresses. This works by explicitly changing the
size of the search box, and allows the code to automatically restrict the range of the parameters to the
part of the space where the best solutions are known to reside. Zooming improves both the rate of
convergence and the accuracy of the final solutions. Ferret can also automatically zoom back out if
solutions begin clustering near the boundary of the search box after zooming has occurred, since this may
indicate that the optimal solution, or at least some part of the optimal set, resides outside of the search
box.
<!--l. 532--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.6   </span> <a 
 id="x8-140002.2.6"></a>Critical Parameter Detection</h4>
<a 
 id="dx8-14001"></a>
<a 
 id="dx8-14002"></a>
<!--l. 537--><p class="noindent" >It is not uncommon in large, non-linear parameter search and optimization problems for the user to initially have an
incomplete understanding of the role that each parameter plays in minimizing the problem&#8217;s fitness
function or functions. For example, some parameters may be much more important than others, or some
parameters may even be nearly irrelevant. Imagine an extreme example of optimizing the settings of an
industrial machine that is controlled by 100 dials, but unbeknownst to the operator, only ten of the dials
actually have a significant effect on the process being optimized. Obviously, the key to solving such a
problem is to discover which dials are important, and forget about the rest, since this would transform
a nearly intractable 100-dimensional problem into a much more manageable problem with only ten
parameters.
                                                                                         

                                                                                         
<!--l. 539--><p class="noindent" >The example given above is obviously a very extreme and artificial case. However, it is not uncommon in real-world
problems to discover that at least <span 
class="cmti-10">some </span>parameters are much less important than others. In my experience, the
ability to reduce the dimensionality of a parameter space by even one or two parameters can make a problem
noticeably easier. On a more theoretical level, if Ferret consistently tells you that a particular parameter of your
model is unimportant, then you should probably try to understand why this is happening, and remove it from the
model if at all possible.
<a 
 id="dx8-14003"></a>
<a 
 id="dx8-14004"></a>
<!--l. 543--><p class="noindent" >Ferret is designed to automatically challenge the importance of parameters during a run whenever its &#8216;Critical
Parameter Detection&#8217; (CPD) feature is turned on. If there are parameters of little importance in the problem, Ferret
very quickly realizes this and effectively reduces the dimensionality of the parameter space by ignoring, or paying
less attention, to this parameter in the search. Ferret&#8217;s interface displays a bar graph that indicates
the qualitative importance of each parameter. More accurately, the height of each bar represents the
number of fully specified copies of the corresponding &#8216;gene&#8217; (or parameter) present in the entire set of
populations.<span class="footnote-mark"><a 
href="QubistHTML10.html#fn2x2"><sup class="textsuperscript">2</sup></a></span><a 
 id="x8-14005f2"></a> 
Where genes are not fully represented, they are encoded internally as <span 
class="cmmi-10">NaN</span>, which Ferret normally
interprets as a random number within the parameter range. Section <a 
href="#x8-140002.2.6">2.2.6<!--tex4ht:ref: sec:CPD --></a> will show you how to set up this
feature.
<a 
 id="dx8-14006"></a>
<!--l. 546--><p class="noindent" >Of course, the relevance of a parameter is ofen contextual. A parameter that is not relevant early in the search may
become more important as the algorithm zooms in on the optimal set, or one that was important
during early evolution may lose its relevance later on. Ferret constantly checks the importance of each
parameter as a run progresses, and dynamically adapts its internal model of parameter importance, as new
regions of the fitness landscape are sampled. Both Anvil and SemiGloSS&#x00A0;contain a weaker form of CPD,
which may be useful on some problems. However, Locust does not contain any feature analogous to
CPD.
<a 
 id="dx8-14007"></a>
<a 
 id="dx8-14008"></a>
<!--l. 550--><p class="noindent" >I have personally found this feature useful in an astrophysical modeling code called GalAPAGOS, developed in
collaboration with my colleague Dr. Jayanne English at the University of Manitoba, which involves
modeling hydrogen gas disks in galaxies [<a 
href="QubistHTMLli1.html#Xfiege07a">Fiege et al.</a>,&#x00A0;<a 
href="QubistHTMLli1.html#Xfiege07a">2007</a>]. For this problem, my colleagues and I have
developed a very large model with 23-26 parameters, depending on how we set it up, that represents the
rotating gas disk of a galaxy. Our task is to fit it to a three-dimensional &#8216;spectral data cube&#8217; with two
spatial dimensions and one velocity dimension, and to map out the region of interest within 1-sigma of
the <span 
class="cmmi-10">&#x03C7;</span><sup><span 
class="cmr-7">2</span></sup> minimum. This is clearly a daunting task. However, we find that the model is simplified for
some data sets, since a significant fraction of the parameters (1/4 to 1/2 of them occasionally) become
unimportant within our 1-sigma noise threshold! Only one or two parameters become unimportant for more
difficult data sets. Nevertheless, the CPD feature is important to the project as a whole, because it
allows us to concentrate our efforts and computing resources on the data sets that are intrinsically
                                                                                         

                                                                                         
difficult.
<a 
 id="dx8-14009"></a>
<h4 class="subsectionHead"><span class="titlemark">2.2.7   </span> <a 
 id="x8-150002.2.7"></a>Linkage Learning</h4>
<a 
 id="dx8-15001"></a>
<a 
 id="dx8-15002"></a>
<!--l. 559--><p class="noindent" >We hear a lot these days about new holistic approaches in medicine, ecosystem management, the environment,
economics, and many other fields. These fields share the property that the system as a whole displays complex
behavior, which is often unexpected, as a result of a complicated network of interactions between its component
parts. This is also a reasonable description of the Qubist optimizers Ferret and Locust (and to a lesser extent Anvil),
because they rely on the emergent properties of a population of interacting individuals or swarming particles to
search a parameter space. I will even go so far as to say that I took a somewhat holistic approach at times in
developing this software, because I spent many hours observing how each subtle change to the dynamics of the
interactions affected the behaviour of the population or swarm as a whole - often without <span 
class="cmti-10">really </span>being able to reduce
the behaviour to a straightforward cause and effect relationship, or being able to fully disentangle it from other
effects.
<a 
 id="dx8-15003"></a>
<a 
 id="dx8-15004"></a>
<!--l. 563--><p class="noindent" >A holistic approach is sometimes warranted for problems of the type discussed above, but a reductionist approach is
often more efficient and reliable for problems that do not involve such complicated networks of interactions between
components parts. Humans naturally solve complex problems by a &#8216;divide &amp; conquer&#8217; strategy that relies on
our intuitive ability to break large problems into smaller, nearly independent sub-problems that can
be solved almost independently and linked together to construct the full solution. This reductionist
approach has been a cornerstone of the scientific method, and centuries of innovation have proven that it
works.
<a 
 id="dx8-15005"></a>
<!--l. 566--><p class="noindent" >A major goal in GA research is to build &#8216;linkage-learning&#8217; algorithms with this reductionist capability. A
linkage-learning algorithm automatically searches for ways to partition problems with many parameters into several
semi-independent problems, which each has a smaller number of parameters. Small to medium-sized problems with
less than about ten to fifteen parameters are often quite tractable, but many problems encountered in science,
engineering, and other technical disciplines are much larger. A linkage learning algorithm offers a powerful new
strategy to solve such problems by mirroring the reductionist approach to innovation developed by humans. It
intelligently partitions big problems with many parameters into smaller and simpler problems, which are often
relatively easy (or at least manageable) on their own, optimizes each component independently or with minimal
interaction with other components, and re-assembles the solution from its fundamental component
parts. Ferret is very good at doing this, and it does so automatically. In the language of the holistic vs.
reductionist discussion above, Ferret takes a reductionist approach wherever it can, and a holistic one
on the highly non-linear sub-problems where this doesn&#8217;t work. Of course, the key is knowing where
to make the division between parameters that can be broken into sub-problems, and those that are
indivisible. This is the job of Ferret&#8217;s linkage learning system, and I am happy to report that it works very
well.
<!--l. 568--><p class="noindent" >The basic idea of Ferret&#8217;s linkage-learning system is simple. Ferret regards two parameters A and B as linked if
non-linear saddle-like behaviour is detected, such that variations of A and B independently result in
worsening of a solution, but the same variations applied together result in improvement. In such cases, it is
                                                                                         

                                                                                         
clear that A and B should be linked so that they are (usually) traded together during crossovers, to
preserve gains made by varying the parameters together. A novel extension of Ferrets linkage-learning
algorithm is its ability to search entire sets of parameters A and B for linkage in parallel, which is
assigned probabilistically to the parameters within these sets. Thus, Ferret treats linkage as a matrix of
probabilities that co-evolves with the population during the search. Parameters that appear linked at
the start of a run may not appear linked at the end, when most solutions may be nearly optimal.
Conversely, new links can also arise as the code explores previously uncharted regions of parameter
space.
<!--l. 570--><p class="noindent" >Ferret&#8217;s linkage-learning algorithm is based on a dynamically evolving internal model of a problem&#8217;s linkage
structure. Like CPD, a problem&#8217;s linkage state is not static, and it is very often the case that parameters that need
to be linked early on in a run can be unlinked at later times. This is an opportunity that should not be missed,
because unlinking may improve the efficiency of the search very substantially. On the other hand, parameters that
were unlinked at early times may need to be linked late in a run when the population begins to converge toward its
final solution set. Linkages must be discovered reliably, since a failure to do so could result in the code settling into a
false local minimum.
<a 
 id="dx8-15006"></a>
<a 
 id="dx8-15007"></a>
<!--l. 574--><p class="noindent" >Ferret constantly tries to refine its linkage map each generation throughout the duration of a run. This occurs while
the optimization is in progress, and is handled entirely automatically, without any user input. However, the evolving
<span 
class="cmti-10">linkage matrix </span>is displayed in the user interface, and this often holds clues about a problem&#8217;s mathematical structure
and level of difficulty, because problems with many linked parameters are more difficult that those with
few linkages. Each generation, Ferret partitions a problem into sub-problems using its best current
model of the linkage structure, performs evolutionary operations on building blocks composed of linked
parameters, as well as other operations that are independent of linkage structure, and re-assembles a full
solution for each parameter set. This process is performed iteratively over all of the generations of the
run.
<a 
 id="dx8-15008"></a>
<a 
 id="dx8-15009"></a>
<!--l. 578--><p class="noindent" >The search for linked &#8216;building blocks&#8217; amounts to a search for certain non-linear mathematical structures in
the parameter space. This search takes place while the parameters themselves are being searched so
that the linkage structure and the parameters co-evolve. Ferret&#8217;s linkage-learning is reliable, even on
problems where the linkages are badly scaled. An example found in Ferret&#8217;s &#8216;Demo&#8217; directory called
&#8216;Linkage-Learning/LinkageLearning-ScaledDeception&#8217; shows how Ferret&#8217;s linkage-learning algorithm behaves on a
very badly scaled problem with thirty parameters, where there are ten building blocks to discover, with three linked
parameters each. These building blocks span ten orders of magnitude in their importance to the fitness
function. Ferret is able to discover all ten building blocks reliably, and moreover, all building blocks are
discovered at the same rate. This performance is outstanding in comparison to other linkage-learning
algorithms, which usually can only find the most important few building blocks in badly scaled problems
[<a 
href="QubistHTMLli1.html#Xgoldberg02">Goldberg</a>,&#x00A0;<a 
href="QubistHTMLli1.html#Xgoldberg02">2002</a>].
<a 
 id="dx8-15010"></a>
<!--l. 581--><p class="noindent" >Ferret is the only global optimization package that I know of, which combines multi-objective optimization, critical
parameter detection, and linkage learning. This makes it well-suited for many problems that are intractable using
other techniques.
<a 
 id="dx8-15011"></a>
<a 
 id="dx8-15012"></a>
                                                                                         

                                                                                         
<!--l. 586--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.8   </span> <a 
 id="x8-160002.2.8"></a>Strategy Auto-Adaptation</h4>
<a 
 id="dx8-16001"></a>
<a 
 id="dx8-16002"></a>
<a 
 id="dx8-16003"></a>
<!--l. 592--><p class="noindent" >In computer science, the &#8216;evolution strategy&#8217; (ES) algorithm is an alternate type of evolutionary optimization
algorithm that differs significantly from a genetic algorithm. Traditional GAs use a binary representation of a
problem&#8217;s parameters, while ES uses a continuous parameter representation. GAs use a combination of mutation,
crossover, and selection operators, while ES uses only mutation and selection, where mutation is usually defined as a
Gaussian random perturbation of a solution&#8217;s real-valued parameters. One advantage of ES over typical GAs is that
the <span 
class="cmti-10">mutation strength </span>is automatically optimized while the algorithm runs, by dynamically evolving the standard
deviation of the mutation perturbations.
<!--l. 594--><p class="noindent" >Ferret is not a traditional genetic algorithm by any means, and borrows heavily from ES. Like ES, Ferret&#8217;s
parameter representation is based on continuous real variables, and the mutation operator is a Gaussian random
perturbation, whose strength evolves during the run. Moreover, Ferret&#8217;s auto-adaptation strategy extends to several
other strategy parameters as well. These include the following:
     <ul class="itemize1">
     <li class="itemize">
     Strength
     of
     the
     crossover
     operator,
     which
     controls
     the
     degree
     of
     mixing
     between
     two
     solutions
     during
     crossover.
     </li>
     <li class="itemize">
     The
     crossover
     &#8216;dispersion&#8217;
     parameter,
                                                                                         

                                                                                         
     which
     controls
     a
     mutation-like
     Gaussian
     perturbation
     applied
     during
     crossover
     operators,
     in
     order
     to
     better
     explore
     and
     fill
     in
     the
     optimal
     set.
     </li>
     <li class="itemize">
     Ferret&#8217;s
     &#8216;mating
     restriction&#8217;
     parameter,
     which
     controls
     whether
     mates
     are
     preferentially
     selected
     from
     nearby
     or
     distant
     parts
     of
     the
     parameter
     space
     during
     crossover.
     </li>
                                                                                         

                                                                                         
     <li class="itemize">
     A
     parameter
     that
     controls
     the
     strength
     of
     Ferret&#8217;s
     &#8216;Advanced
     Lethal
     Suppression&#8217;
     algorithm,
     discussed
     in
     Section
     <a 
href="#x8-170002.2.9">2.2.9<!--tex4ht:ref: sec:ALS --></a>.
     </li>
     <li class="itemize">
     A
     parameter
     that
     controls
     Ferret&#8217;s
     &#8216;niching
     acceleration&#8217;
     system,
     which
     helps
     Ferret
     to
     infer
     &#8216;good&#8217;
     directions
     to
     explore
     from
     the
     current
     spatial
     distribution
     of
     the
     population
     (Section
                                                                                         

                                                                                         
     <a 
href="QubistHTMLse52.html#x78-1130008.9.3">8.9.3<!--tex4ht:ref: sec:acceleration --></a>).</li></ul>
<a 
 id="dx8-16004"></a>
<!--l. 604--><p class="noindent" >With real-valued parameters, heavy borrowing from ES techniques, and all of these fancy techniques discussed
above, I would not be all that surprised to hear comments from GA purists that Ferret is not <span 
class="cmti-10">really </span>a GA. You
won&#8217;t hear any argument from me because this is absolutely true. However, I still refer to Ferret as a
GA because the crossover operator plays a central role in this code, which is a defining feature of
GAs. I take a pragmatic view on this topic and I don&#8217;t <span 
class="cmti-10">really </span>care whether Ferret is a true genetic
algorithm or not, because the features that make its classification as a genetic algorithm questionable
also help to make it so powerful as an optimizer. Ferret&#8217;s development has always been driven by the
scientific problems that I want to solve, and if the code is no longer a &#8216;real&#8217; genetic algorithm as a
result, then that is fine with me. I will therefore use the term GA rather loosely when talking about
Ferret, since this code is clearly very different from classical GAs or what others may refer to a GA
in the literature. These differences will be expounded upon fully within the remaining pages of this
guide.
<a 
 id="dx8-16005"></a>
<!--l. 608--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.9   </span> <a 
 id="x8-170002.2.9"></a>Advanced Lethal Suppression (ALS)</h4>
<a 
 id="dx8-17001"></a>
<a 
 id="dx8-17002"></a>
<a 
 id="dx8-17003"></a>
<!--l. 614--><p class="noindent" >Lethals are poor solutions that are sometimes formed during the crossover of two high-quality parents. Ferret&#8217;s
Advanced Lethal Suppression (ALS) algorithm is a unique innovation that allows Ferret to learn what parts of the
parameter space are likely to contain lethals, and to avoid these regions. This can improve the efficiency of the
algorithm significantly, since fewer lethals are formed and fewer solutions are therefore wasted. This feature is not
necessary for most problems, but it is worth trying if you suspect a problem with lethals. Usually, I do not
turn it on explicitly, but I do allow it to evolve using the auto-adaptation feature discussed in Section
<a 
href="#x8-160002.2.8">2.2.8<!--tex4ht:ref: sec:strategy --></a>.
<!--l. 616--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.10   </span> <a 
 id="x8-180002.2.10"></a>Pausing, Stopping, and Resuming Runs</h4>
<a 
 id="dx8-18001"></a>
<a 
 id="dx8-18002"></a>
<a 
 id="dx8-18003"></a>
<a 
 id="dx8-18004"></a>
<!--l. 623--><p class="noindent" >I live in Manitoba, and we experience violent lightning storms here that knock out the power - and my Ferret runs -
a few times each summer. Many organizations also seem especially fond of &#8216;planned power outages&#8217; that occur with
a few days notice at apparently random intervals throughout the year. Most MATLAB users have
experienced crashes now and then, and many of us know that system administrators often like to time
network maintenance and computer reboots for the middle of two-week long calculations. All cynicism
                                                                                         

                                                                                         
aside, my point is that unexpected things can and all too frequently do happen during calculations
that last longer than a few days. I <span 
class="cmti-10">hate </span>losing results when my Ferret run crashes, and fortunately I
don&#8217;t.
<a 
 id="dx8-18005"></a>
<a 
 id="dx8-18006"></a>
<!--l. 627--><p class="noindent" >During a run, Ferret writes &#8216;History&#8217; files every few generations, which contain a complete record of the best
solutions encountered during the run at each generation. If a run crashes for any reason, it is simple to resume it
with no (or very minimal) data loss. Likewise, you can suspend a run at any time and restart it later, even on a
different machine. It is even possible to modify most strategy parameters between stopping and restarting a
run.<span class="footnote-mark"><a 
href="QubistHTML11.html#fn3x2"><sup class="textsuperscript">3</sup></a></span><a 
 id="x8-18007f3"></a> 
<!--l. 629--><p class="noindent" >I place a lot of emphasis on the ability to pause, stop, and resume runs reliably. This is absolutely
critical for long calculations, and this capability has been a robust feature of Ferret since the earliest
version of the code. I have also ported these features to Locust, since it is also suitable for long-duration
calculations.
<h4 class="subsectionHead"><span class="titlemark">2.2.11   </span> <a 
 id="x8-190002.2.11"></a>Analysis and Integration of Polishers</h4>
<a 
 id="dx8-19001"></a>
<!--l. 635--><p class="noindent" >At the end of a run, the user normally clicks the &#8216;Analyze&#8217; button on the user interface, which causes
the code to load and examine all of the saved History files, compare every solution saved with every
other solution, and construct the final optimal set. This may contain a single solution for a simple
single-objective problem. Alternatively, it may contain thousands of solutions spanning the optimal
trade-off surface of a multi-objective problem, or a similarly large number of solutions distributed within
the optimal region of a fuzzy single-objective problem, where a non-zero &#8216;fuzzy&#8217; tolerance has been
set.
<a 
 id="dx8-19002"></a>
<!--l. 638--><p class="noindent" >Genetic algorithms are very good at finding global solutions, but these solutions are often not obtained to high
precision. Ferret addresses this problem with several solution &#8216;polishers&#8217; based on MATLAB&#8217;s built-in fminsearch
routine (for single-objective problems only), or the Qubist algorithms SAMOSA, Anvil or SemiGloSS which can be
used for single or multi-objective problems. The combination of Ferret plus a solution polisher offers the best of both
worlds: a powerful multi-objective global optimizer, with enhanced capabilities for obtaining final solutions to
high-accuracy.
<a 
 id="dx8-19003"></a>
<!--l. 643--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.12   </span> <a 
 id="x8-200002.2.12"></a>Integrated Visualization</h4>
<a 
 id="dx8-20001"></a>
                                                                                         

                                                                                         
<a 
 id="dx8-20002"></a>
<a 
 id="dx8-20003"></a>
<a 
 id="dx8-20004"></a>
<!--l. 650--><p class="noindent" >My goal in developing Qubist is not only to find optimal solutions, but also to provide tools to help understand
what they mean by exploring the mathematical and geometrical structure of the optimal set. The Qubist
package contains fully integrated and intuitive visualization software that allows you to view any two or
three-dimensional projection of the optimal set as a scatter plot, image or contour plot, though a sophisticated
imaging interface that allows the user to tease interesting features out of the data set. You can also
&#8216;paint&#8217; regions of interest, view them from different orientations, save them to disk, and incorporate
your own graphics right into the visualization interface. User-modifiable publication-quality figures
are produced by the interface automatically by selecting the &#8216;Take Snapshot&#8217; item from a menu. All
user interface components are actually distributed as user-modifiable &#8216;skins&#8217; - if you don&#8217;t like how
something in the interface works or want to add a component for your project, then you are welcome to do
so.
<a 
 id="dx8-20005"></a>
<a 
 id="dx8-20006"></a>
<!--l. 654--><p class="noindent" >Visualization of the optimal set is a cornerstone of the Qubist package and a current area of very active
development. I plan to release an especially powerful new tool for exploring multi-objective optimal sets, tentatively
called &#8216;nQuest&#8217;, as an update within a few months of this writing. The existing visualization tools are the topic of
Chapter <a 
href="QubistHTMLch5.html#x49-730005">5<!--tex4ht:ref: ch:visualization --></a>.
<a 
 id="dx8-20007"></a>
                                                                                         

                                                                                         
<!--l. 658--><div class="crosslinks"><p class="noindent">[<a 
href="QubistHTMLse3.html" >next</a>] [<a 
href="QubistHTMLse1.html" >prev</a>] [<a 
href="QubistHTMLse1.html#tailQubistHTMLse1.html" >prev-tail</a>] [<a 
href="QubistHTMLse2.html" >front</a>] [<a 
href="QubistHTMLch2.html#QubistHTMLse2.html" >up</a>] </p></div>
<!--l. 658--><p class="noindent" ><a 
 id="tailQubistHTMLse2.html"></a>  
</body></html> 
